{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "### Data Science Bowl 2019/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set(style='darkgrid')\n",
    "plt.style.use('dark_background')\n",
    "pd.set_option('max_colwidth', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train: (11341042, 11)\n",
      "Shape of test: (1156414, 11)\n",
      "Shape of labels: (17690, 7)\n",
      "Shape of specs: (386, 3)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "labels = pd.read_csv('./data/train_labels.csv')\n",
    "specs = pd.read_csv('./data/specs.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "\n",
    "print(\"Shape of train: {}\".format(train.shape))\n",
    "print(\"Shape of test: {}\".format(test.shape))\n",
    "print(\"Shape of labels: {}\".format(labels.shape))\n",
    "print(\"Shape of specs: {}\".format(specs.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Cleaning, Sorting, Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listing these vars which will be used to filter columns\n",
    "games = list(train[train['type']=='Game']['title'].unique())\n",
    "activities = list(train[train['type']=='Activity']['title'].unique())\n",
    "assessments = list(train[train['type']=='Assessment']['title'].unique())\n",
    "types = ['Game', 'Activity', 'Assessment']\n",
    "worlds = ['MAGMAPEAK', 'TREETOPCITY', 'CRYSTALCAVES']\n",
    "\n",
    "titles = games + activities + assessments\n",
    "dont_care = [i for i in train['title'].unique() if i not in titles]\n",
    "dont_care.append('NONE')\n",
    "dont_care.append('Clip')\n",
    "\n",
    "# setting dict to map install ids to game sessions\n",
    "install_dict_train = dict(zip(list(train['game_session']), list(train['installation_id'])))\n",
    "install_dict_test = dict(zip(list(test['game_session']), list(test['installation_id'])))\n",
    "\n",
    "# labels of game sessions for mapping at the end\n",
    "labeled_sessions = list(labels['game_session'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_time(df):\n",
    "    '''\n",
    "    This function does three things to fix the time variables:\n",
    "    a) converts timestamp column to datetime\n",
    "    b) converts game time from milliseconds to seconds\n",
    "    c) calculates the individual time taken for each event/row\n",
    "    '''\n",
    "    # convert timestamp to timestamp\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    # converts milliseconds to seconds\n",
    "    df['game_time_s'] = df['game_time'] / 1000\n",
    "    # calculates the time for each game step, replaces negatives with 0\n",
    "    df['step_time'] = df['game_time_s'].diff().clip_lower(0)\n",
    "    df['step_time'].fillna(0, inplace = True)\n",
    "    df.drop(columns = ['game_time', 'game_time_s'], inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to filter to assessed only\n",
    "def filter_assessed(train_df, labels_df):\n",
    "    '''\n",
    "    This function takes the train dataframe and labels dataframe and filters the training\n",
    "    data to only include install ids that are in the train labels.\n",
    "    '''\n",
    "    install_ids = list(labels_df['installation_id'].unique())\n",
    "    new_df = train_df[train_df['installation_id'].isin(install_ids)]\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_time(df):\n",
    "    '''\n",
    "    This function is very important for calculating the cumulative sum of events leading up\n",
    "    to the current game session: it sorts the dataframe by installation id and timestamp\n",
    "    '''\n",
    "    new_df = df.sort_values(by = ['installation_id', 'timestamp'], ascending = True)\n",
    "    new_df.reset_index(inplace = True)\n",
    "    new_df.drop(columns = ['index'], inplace =True)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Features on Full (Unaggregated) Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_answers(df, code = [4100, 4110]):\n",
    "    '''\n",
    "    This function calculates correct/incorrect answers (for assessments). Bird Measurer uses\n",
    "    code 4110; all other assessments use code 4100.\n",
    "    '''\n",
    "    # assessments\n",
    "    answers = df[(df['event_code'] == code)]\n",
    "    correct = [1 if '\"correct\":true' in i else 0 for i in answers['event_data']]\n",
    "    incorrect = [1 if '\"correct\":false' in i else 0 for i in answers['event_data']]\n",
    "    \n",
    "    correct_dict = dict(zip(list(answers.index), correct))\n",
    "    incorrect_dict = dict(zip(list(answers.index), incorrect))\n",
    "\n",
    "    df['test_correct'] = df.index.map(correct_dict).fillna(0)\n",
    "    df['test_incorrect'] = df.index.map(incorrect_dict).fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def test_answers_df(df):\n",
    "    '''\n",
    "    This is the wrapper function for the test answers function; it splits the Bird\n",
    "    Measurer Assessment from the others; calculates correct/incorrect responses, and \n",
    "    rejoins the df.\n",
    "    '''\n",
    "    # splitting into birds/not birds\n",
    "    notbirds_lab = test_answers((df[df['title']!='Bird Measurer (Assessment)']), 4100)\n",
    "    birds_lab = test_answers((df[df['title']=='Bird Measurer (Assessment)']), 4110)\n",
    "    # re-joining\n",
    "    all_lab = pd.concat([notbirds_lab, birds_lab], axis = 0)\n",
    "    \n",
    "    return all_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_events(df):\n",
    "    '''\n",
    "    This function one-hot encodes specific game events so they can be counted/aggregated.\n",
    "    Game events include: true/false game responses, positive/negative feedback from games,\n",
    "    whether the player got an instruction, whether the player requested help, and whether the\n",
    "    player beat the level.\n",
    "    '''\n",
    "    # whether a player did something correct/incorrect in a game\n",
    "    df['session_true'] = [1 if '\"correct\":true' in i else 0 for i in df['event_data']]\n",
    "    df['session_false'] = [1 if '\"correct\":false' in i else 0 for i in df['event_data']]\n",
    "    # whether player got positive/negative feedback on a move\n",
    "    df['session_goodjob'] = [1 if i == 3021 else 0 for i in df['event_code']]\n",
    "    df['session_tryagain'] = [1 if i == 3020 else 0 for i in df['event_code']]\n",
    "    # whether a player got extra instructions\n",
    "    df['got_instructions'] = [1 if i == 3010 else 0 for i in df['event_code']]\n",
    "    # whether a player got help\n",
    "    df['requested_help'] = [1 if i == 4090 else 0 for i in df['event_code']]\n",
    "    # whether beat the level\n",
    "    df['beat_level'] = [1 if i == 2050 else 0 for i in df['event_code']]\n",
    "    df.drop(columns=['event_count', 'event_code'], inplace =True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_session(df):\n",
    "    '''\n",
    "    This function marks the beginning of a new game session. This is important for doing any \n",
    "    cumulative/aggregate calculations before aggregating the df. This function is called in \n",
    "    the activity_df function. \n",
    "    '''\n",
    "    # marking the beginning of a new game session\n",
    "    game_sessions = list(df['game_session'].unique())\n",
    "    sessions_dict = dict(zip(game_sessions, list(range(len(game_sessions)))))\n",
    "    df['new_session_id'] = df['game_session'].map(sessions_dict)\n",
    "    df['new_game'] = df['new_session_id'].diff()\n",
    "    df['new_game'] = [1 if i >= 1 else 0 for i in df['new_game']]\n",
    "    df['new_game'].fillna(1, inplace = True)\n",
    "    df['start_game'] = df['new_game'] * df['timestamp'].map(str)\n",
    "    df.drop(columns=['new_session_id'], inplace =True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activity_df(df):\n",
    "    '''\n",
    "    This function dummies out the type, title, and world of the current game session and\n",
    "    multiplies the dummy features by 'new game' so the 'total' dummy after aggregating each\n",
    "    of these features with the rest of the dataframe is either 0 or 1. (Ex. instead of counting\n",
    "    every step/row in the df as 'MAGMA PEAK' and summing to 247, we would only count it once.)\n",
    "    '''\n",
    "    #This function should be called after the 'new session' feature has been created.\n",
    "    df = new_session(df)\n",
    "    # preserve list of original columns\n",
    "    original_cols = list(df.columns)\n",
    "    # get lists of games, activities, assessments                    \n",
    "    df = pd.get_dummies(df, columns=['type', 'title', 'world'])\n",
    "    # all the numeric columns we want to be able to count over time\n",
    "    new_cols = list(df.columns)\n",
    "    # list of new dummied columns\n",
    "    new_dum = [i for i in new_cols if i not in original_cols]\n",
    "    # pruning columns\n",
    "    drop = []\n",
    "    for n in new_dum:\n",
    "        for d in dont_care:\n",
    "            if d in n:\n",
    "                drop.append(n)\n",
    "    # drop unnecessary dummy columns\n",
    "    df.drop(columns=drop, inplace = True)\n",
    "    # multiple dummy columns by counts\n",
    "    new_dum2 = [i for i in new_dum if i not in drop]\n",
    "    for i in new_dum2:\n",
    "        df[i] = df[i] * df['new_game']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def start_time(df):\n",
    "    df1 = df[df['new_game']==1]\n",
    "    start_time_dict = dict(zip(list(df1['game_session']), list(df1['start_game'])))\n",
    "    \n",
    "    return start_time_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Aggregating by Game Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_session(df):\n",
    "    '''\n",
    "    This function takes in a dataframe with one-hot encoding/all other features already\n",
    "    calculated on the original rows and groups them by game session. Only do this after\n",
    "    done creating features on the original df!\n",
    "    '''\n",
    "    sum_agg = df.groupby('game_session').sum()\n",
    "    count_steps = df.groupby('game_session').count()['new_game']\n",
    "    sum_agg['session_steps'] = count_steps\n",
    "    sum_agg.reset_index(inplace = True)\n",
    "    \n",
    "    return sum_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(df):\n",
    "    '''\n",
    "    This function takes an aggregated (grouped by) dataframe and calculates the test \n",
    "    accuracy and game accuracy of each game session.\n",
    "    '''\n",
    "    df['test_accuracy'] = df['test_correct'] / (df['test_correct'] + df['test_incorrect'])\n",
    "    df['game_accuracy'] = df['session_true'] / (df['session_true'] + df['session_false'])\n",
    "    df['feedback_positive'] = df['session_goodjob'] / (df['session_goodjob'] + df['session_tryagain'])\n",
    "    df.fillna(0, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_features(df):\n",
    "    '''\n",
    "    This function maps features for the game session to the title/type/world they were counted\n",
    "    in (Ex. the game session's total time would be recorded as time spent in Magma Peak etc.)\n",
    "    '''\n",
    "    all_cols = list(df.columns)\n",
    "    dum_cols = []\n",
    "    for i in df.columns:\n",
    "        if \"title_\" in i or \"world_\" in i or \"type_\" in i:\n",
    "            dum_cols.append(i)\n",
    "    not_dum = [x for x in all_cols if x not in dum_cols]\n",
    "    num_cols = []\n",
    "    for i in not_dum:\n",
    "        if df[i].dtype == 'int64' or df[i].dtype == 'float64':\n",
    "            num_cols.append(i)\n",
    "    num_cols.remove('new_game')\n",
    "    for i in num_cols:\n",
    "        for k in dum_cols:\n",
    "            new_col = k + '_' + i\n",
    "            df[new_col] = df[i].map(int) * df[k].map(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Aggregating by Installation ID (over time)\n",
    "Functions for calculating cumulative sums up until a game session, and calculating most recent scores, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_install_start(df, install_dict, start_dict):\n",
    "    '''\n",
    "    This function maps the installation ids back to our df aggregated by game session.\n",
    "    '''\n",
    "    df['installation_id'] = df['game_session'].map(install_dict)\n",
    "    df['start_game'] = df['game_session'].map(start_dict)\n",
    "    df['start_game'] = pd.to_datetime(df['start_game'])\n",
    "    df = df.sort_values(by = ['installation_id', 'start_game'], ascending = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_install(df):\n",
    "    '''\n",
    "    This function calculates cumulative sums on the aggregated df by game session.\n",
    "    This is important for being able to calculate all prior information to the game session\n",
    "    in the training labels/installation id in the test data.\n",
    "    '''\n",
    "    cols = []\n",
    "    for i in df.columns:\n",
    "        if 'accuracy' in i or 'feedback' in i:\n",
    "            pass\n",
    "        elif df[i].dtype == 'int64' or df[i].dtype == 'float64':\n",
    "            cols.append(i)\n",
    "    for i in cols:\n",
    "        cuma = df.groupby('installation_id')[i].cumsum(skipna = True).shift()\n",
    "        cuma_col = i + '_cuma'\n",
    "        df[cuma_col] = cuma\n",
    "    df.fillna(0, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  E. Post-Cumulative/Agg Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_aggs(df):\n",
    "    keep = ['feedback_positive', 'game_accuracy', 'test_accuracy']\n",
    "    title_cols = [i for i in df.columns if 'title_' in i and '_cuma' in i]\n",
    "    world_cols = [i for i in df.columns if 'world_' in i and'_cuma' in i]\n",
    "    type_cols = [i for i in df.columns if 'type_' in i and '_cuma' in i]\n",
    "    dum_cols = title_cols + world_cols + type_cols\n",
    "\n",
    "    redo = []\n",
    "    for i in dum_cols:\n",
    "        for m in keep:\n",
    "            if m in i:\n",
    "                redo.append(i)\n",
    "    df.drop(columns = redo, inplace = True)\n",
    "    \n",
    "    eventcount = [i for i in df.columns if \"event_count\" in i]\n",
    "    eventcode = [i for i in df.columns if \"event_code\" in i]\n",
    "    \n",
    "    df.drop(columns=eventcount, inplace = True)\n",
    "    df.drop(columns=eventcode, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-calculating Accuracy (for CumSum columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterables = ['type_Activity', 'type_Assessment', 'type_Game', 'title_Air Show',\n",
    "                 'title_All Star Sorting', 'title_Bird Measurer (Assessment)',\n",
    "                 'title_Bottle Filler (Activity)', 'title_Bubble Bath',\n",
    "                 'title_Bug Measurer (Activity)', 'title_Cart Balancer (Assessment)',\n",
    "                 'title_Cauldron Filler (Assessment)', 'title_Chest Sorter (Assessment)',\n",
    "                 'title_Chicken Balancer (Activity)', 'title_Chow Time', 'title_Crystals Rule',\n",
    "                 'title_Dino Dive', 'title_Dino Drink', 'title_Egg Dropper (Activity)',\n",
    "                 'title_Fireworks (Activity)', 'title_Flower Waterer (Activity)', \n",
    "                 'title_Happy Camel', 'title_Leaf Leader', 'title_Mushroom Sorter (Assessment)',\n",
    "                 'title_Pan Balance', 'title_Sandcastle Builder (Activity)',\n",
    "                 'title_Scrub-A-Dub', 'title_Watering Hole (Activity)', 'world_CRYSTALCAVES',\n",
    "                 'world_MAGMAPEAK', 'world_TREETOPCITY']\n",
    "test_iterables = ['type_Assessment', 'title_Bird Measurer (Assessment)',\n",
    "                 'title_Cart Balancer (Assessment)', 'title_Cauldron Filler (Assessment)', \n",
    "                  'title_Chest Sorter (Assessment)', 'title_Mushroom Sorter (Assessment)',\n",
    "                  'world_CRYSTALCAVES','world_MAGMAPEAK', 'world_TREETOPCITY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_accuracy(df, iterables = [train_iterables, test_iterables]):\n",
    "\n",
    "    for i in iterables:\n",
    "        tc = i + '_test_correct_cuma'\n",
    "        ti = i + '_test_incorrect_cuma'\n",
    "    \n",
    "        st = i + '_session_true_cuma'\n",
    "        sf = i + '_session_false_cuma'\n",
    "    \n",
    "        gj = i + '_session_goodjob_cuma'\n",
    "        ta = i + '_session_tryagain_cuma'\n",
    "    \n",
    "        test_acc = i + '_test_accuracy_cuma'\n",
    "        game_acc = i + '_game_accuracy_cuma'\n",
    "        feedback = i + '_positive_feedback_cuma'\n",
    "    \n",
    "\n",
    "        df[test_acc] = df[tc] / (df[tc] + df[ti])\n",
    "        df[game_acc] = df[st] / (df[st] + df[sf])\n",
    "        df[feedback] = df[gj] / (df[gj] + df[ta])\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping/Filling CumSum Null Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_fill_cumsum(df):\n",
    "    allnull = []\n",
    "    for i in df.columns:\n",
    "        if df[i].isnull().sum() == df.shape[0]:\n",
    "            allnull.append(i)\n",
    "    df.drop(columns=allnull, inplace = True)\n",
    "    df = df.groupby(['installation_id'], as_index=False).apply(lambda group: group.ffill())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proportion of time spent on each activity/world (cumulative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuma_proportions(df):\n",
    "    time_iters = ['type_Activity_step_time_cuma', 'type_Assessment_step_time_cuma',\n",
    "                  'type_Game_step_time_cuma', 'world_CRYSTALCAVES_step_time_cuma',\n",
    "                  'world_MAGMAPEAK_step_time_cuma', 'world_TREETOPCITY_step_time_cuma']\n",
    "    for i in time_iters:\n",
    "        new_col = i + '_proportion'\n",
    "        df[new_col] = df[i] / df['step_time_cuma']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proportion of Assessments Taken  (cumulative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assessments_completed(df):\n",
    "    assessments_count = ['title_Bird Measurer (Assessment)_cuma',\n",
    "                         'title_Cart Balancer (Assessment)_cuma',\n",
    "                         'title_Cauldron Filler (Assessment)_cuma',\n",
    "                         'title_Chest Sorter (Assessment)_cuma',\n",
    "                         'title_Mushroom Sorter (Assessment)_cuma']\n",
    "    completed = 0\n",
    "    for i in assessments_count:\n",
    "        completed += df[i].astype(bool).astype(int)\n",
    "    try:\n",
    "        done = completed / 5\n",
    "    except:\n",
    "        done = 0\n",
    "    df['assessments_completed'] = done\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proportion of Assessments Passed (cumulative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assessments_passed(df):\n",
    "    \n",
    "    test_correct = ['title_Bird Measurer (Assessment)_test_correct_cuma',\n",
    "                    'title_Cart Balancer (Assessment)_test_correct_cuma',\n",
    "                    'title_Cauldron Filler (Assessment)_test_correct_cuma',\n",
    "                    'title_Chest Sorter (Assessment)_test_correct_cuma',\n",
    "                    'title_Mushroom Sorter (Assessment)_test_correct_cuma']\n",
    "    beat = 0\n",
    "    for i in test_correct:\n",
    "        beat += df[i].astype(bool).astype(int)\n",
    "    try:\n",
    "        passed = beat / 5\n",
    "    except:\n",
    "        passed = 0\n",
    "    df['assessments_passed'] = passed\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time per step: Speed (not cumulative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speed(df):\n",
    "    df['speed'] = df['step_time'] / df['session_steps']\n",
    "    df['speed_cuma'] = df['step_time_cuma'] / df['session_steps_cuma']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Marking Start of New Installation Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_session_install(df):\n",
    "    '''\n",
    "    This function marks the beginning of a new installation id record. \n",
    "    '''\n",
    "    # marking the beginning of a new game session\n",
    "    install_sessions = list(df['installation_id'].unique())\n",
    "    sessions_dict = dict(zip(install_sessions, list(range(len(install_sessions)))))\n",
    "    df['new_install_session_id'] = df['installation_id'].map(sessions_dict)\n",
    "    df['new_install'] = df['new_install_session_id'].diff()\n",
    "    df['new_install'] = [1 if i >= 1 else 0 for i in df['new_install']]\n",
    "    df['new_install'].fillna(1, inplace = True)\n",
    "    df.drop(columns=['new_install_session_id'], inplace =True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Long Since the Last Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timedelta(df):\n",
    "    # deleting weird mistake rows for datetime\n",
    "    zero = df[df['start_game']==0].index\n",
    "    df.drop(zero, axis = 0, inplace = True)\n",
    "    # shifting start game to datetime\n",
    "    df['start_game'] = pd.to_datetime(df['start_game'])\n",
    "    # calculating time difference\n",
    "    df['time_difference'] = df['start_game'].diff()\n",
    "    # removing where install is new\n",
    "    df['time_difference'] = np.where((df['new_install']==1), 0, df['time_difference'])\n",
    "    df['time_difference'] = pd.to_timedelta(df['time_difference'])\n",
    "    df['time_difference'] = [((i.seconds//60)%60) for i in df['time_difference']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mark high-use devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_use(df):\n",
    "    uses = df['installation_id'].value_counts().to_dict()\n",
    "    df['uses'] = df['installation_id'].map(uses)\n",
    "    df['very_high_use'] = [1 if x > 950 else 0 for x in df['uses']]\n",
    "    df['very_low_use'] = [1 if x < 10 else 0 for x in df['uses']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last Scores (not cumulative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_test_scores(df):\n",
    "    \n",
    "    tested_index = df[(df['type_Assessment'] == 1)].index\n",
    "    tested_vals = df.loc[tested_index, 'test_accuracy'].to_dict()\n",
    "    \n",
    "    for i in range(len(tested_index)):\n",
    "        index = tested_index[i]\n",
    "        \n",
    "        # last score\n",
    "        last_index = tested_index[i-1]\n",
    "        last_score = tested_vals[last_index]\n",
    "        df.loc[index, 'last_test_score'] = np.where((df.loc[index, 'new_install'] != 1), \n",
    "                                               last_score, -10.0)\n",
    "        \n",
    "        # one score before last score\n",
    "        two_last_index = tested_index[i-2]\n",
    "        two_last_score = tested_vals[two_last_index]\n",
    "        df.loc[index, 'last_two_test_score'] = np.where((df.loc[index-1, 'new_install'] != 1) and\n",
    "                                           (df.loc[index, 'new_install'] != 1), \n",
    "                                                  two_last_score, -10.0)\n",
    "        try:\n",
    "            # two scores before last score\n",
    "            three_last_index = tested_index[i-3]\n",
    "            three_last_score = tested_vals[three_last_index]\n",
    "            df.loc[index, 'last_three_test_score'] = np.where((df.loc[index-2, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-1, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index, 'new_install'] != 1), \n",
    "                                                    three_last_score, -10.0)\n",
    "        except:\n",
    "            df.loc[index, 'last_three_test_score'] = -10.0\n",
    "        \n",
    "        try:\n",
    "            # three scores before last score\n",
    "            four_last_index = tested_index[i-4]\n",
    "            four_last_score = tested_vals[four_last_index]\n",
    "            df.loc[index, 'last_four_test_score'] = np.where((df.loc[index-3, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-2, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-1, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index, 'new_install'] != 1), \n",
    "                                                    four_last_score, -10.0)\n",
    "            \n",
    "        except:\n",
    "            df.loc[index, 'last_four_test_score'] = -10.0\n",
    "            \n",
    "        try:   \n",
    "            # four scores before last score\n",
    "            five_last_index = tested_index[i-5]\n",
    "            five_last_score = tested_vals[five_last_index]\n",
    "            df.loc[index, 'last_five_test_score'] = np.where((df.loc[index-4, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-3, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-2, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-1, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index, 'new_install'] != 1), \n",
    "                                                    five_last_score, -10.0)\n",
    "        except:\n",
    "            df.loc[index, 'last_five_test_score'] = -10.0\n",
    "        \n",
    "        try:\n",
    "            # five scores before last score\n",
    "            six_last_index = tested_index[i-6]\n",
    "            six_last_score = tested_vals[six_last_index]\n",
    "            df.loc[index, 'last_six_test_score'] = np.where((df.loc[index-5, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-4, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-3, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-2, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-1, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index, 'new_install'] != 1), \n",
    "                                                    six_last_score, -10.0)\n",
    "        except:\n",
    "            df.loc[index, 'last_six_test_score'] = -10.0\n",
    "        \n",
    "        try:\n",
    "            # six scores before last score\n",
    "            seven_last_index = tested_index[i-7]\n",
    "            seven_last_score = tested_vals[seven_last_index]\n",
    "            df.loc[index, 'last_seven_test_score'] = np.where((df.loc[index-6, 'new_install'] != 1) and\n",
    "                                                     (df.loc[index-5, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-4, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-3, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-2, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-1, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index, 'new_install'] != 1), \n",
    "                                                    seven_last_score, -10.0)\n",
    "        except:\n",
    "            df.loc[index, 'last_seven_test_score'] = -10.0\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last Game Accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_game_scores(df):\n",
    "\n",
    "    tested_index = df[(df['type_Game'] == 1) | (df['type_Activity'] == 1)].index\n",
    "    tested_vals = df.loc[tested_index, 'game_accuracy'].to_dict()\n",
    "    \n",
    "    for i in range(len(tested_index)):\n",
    "        index = tested_index[i]\n",
    "        \n",
    "        # last score\n",
    "        last_index = tested_index[i-1]\n",
    "        last_score = tested_vals[last_index]\n",
    "        df.loc[index, 'last_score'] = np.where((df.loc[index, 'new_install'] != 1), \n",
    "                                               last_score, -10.0)\n",
    "        try:\n",
    "            # one score before last score\n",
    "            two_last_index = tested_index[i-2]\n",
    "            two_last_score = tested_vals[two_last_index]\n",
    "            df.loc[index, 'last_two_score'] = np.where((df.loc[index-1, 'new_install'] != 1) and\n",
    "                                           (df.loc[index, 'new_install'] != 1), \n",
    "                                                       two_last_score, -10.0)\n",
    "        except:\n",
    "            df.loc[index, 'last_two_score'] = -10.0\n",
    "            \n",
    "        try:\n",
    "            # two scores before last score\n",
    "            three_last_index = tested_index[i-3]\n",
    "            three_last_score = tested_vals[three_last_index]\n",
    "            df.loc[index, 'last_three_score'] = np.where((df.loc[index-2, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-1, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index, 'new_install'] != 1), \n",
    "                                                    three_last_score, -10.0)\n",
    "        except:\n",
    "            df.loc[index, 'last_three_score'] = -10.0\n",
    "        \n",
    "        try:\n",
    "            # three scores before last score\n",
    "            four_last_index = tested_index[i-4]\n",
    "            four_last_score = tested_vals[four_last_index]\n",
    "            df.loc[index, 'last_four_score'] = np.where((df.loc[index-3, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-2, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-1, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index, 'new_install'] != 1), \n",
    "                                                    four_last_score, -10.0)\n",
    "            \n",
    "        except:\n",
    "            df.loc[index, 'last_four_score'] = -10.0\n",
    "            \n",
    "        try:   \n",
    "            # four scores before last score\n",
    "            five_last_index = tested_index[i-5]\n",
    "            five_last_score = tested_vals[five_last_index]\n",
    "            df.loc[index, 'last_five_score'] = np.where((df.loc[index-4, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-3, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-2, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-1, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index, 'new_install'] != 1), \n",
    "                                                    five_last_score, -10.0)\n",
    "        except:\n",
    "            df.loc[index, 'last_five_score'] = -10.0\n",
    "        \n",
    "        try:\n",
    "            # five scores before last score\n",
    "            six_last_index = tested_index[i-6]\n",
    "            six_last_score = tested_vals[six_last_index]\n",
    "            df.loc[index, 'last_six_score'] = np.where((df.loc[index-5, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-4, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-3, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-2, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-1, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index, 'new_install'] != 1), \n",
    "                                                    six_last_score, -10.0)\n",
    "        except:\n",
    "            df.loc[index, 'last_six_score'] = -10.0\n",
    "        \n",
    "        try:\n",
    "            # six scores before last score\n",
    "            seven_last_index = tested_index[i-7]\n",
    "            seven_last_score = tested_vals[seven_last_index]\n",
    "            df.loc[index, 'last_seven_score'] = np.where((df.loc[index-6, 'new_install'] != 1) and\n",
    "                                                     (df.loc[index-5, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-4, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-3, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-2, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index-1, 'new_install'] != 1) and\n",
    "                                                    (df.loc[index, 'new_install'] != 1), \n",
    "                                                    seven_last_score, -10.0)\n",
    "        except:\n",
    "            df.loc[index, 'last_seven_score'] = -10.0\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Last Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_nans(df):\n",
    "    df.replace(-10.0, np.nan, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Averages of Last Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(df):\n",
    "    \n",
    "    # setting game acc variables\n",
    "    gi = df['last_score']\n",
    "    gii = df['last_two_score']\n",
    "    giii = df['last_three_score']\n",
    "    giv = df['last_four_score']\n",
    "    gv = df['last_five_score']\n",
    "    gvi = df['last_six_score']\n",
    "    gvii = df['last_seven_score']\n",
    "    # setting test acc variables\n",
    "    ti = df['last_test_score']\n",
    "    tii = df['last_two_test_score']\n",
    "    tiii = df['last_three_test_score']\n",
    "    tiv = df['last_four_test_score']\n",
    "    tv = df['last_five_test_score']\n",
    "    tvi = df['last_six_test_score']\n",
    "    tvii = df['last_seven_test_score']\n",
    "    # averages\n",
    "    try:\n",
    "        df['two_avg'] = (gi + gii) / 2\n",
    "    except:\n",
    "        df['two_avg'] = 9999\n",
    "    try:\n",
    "        df['three_avg'] = (gi + gii + giii) / 3\n",
    "    except:\n",
    "        df['three_avg'] = 9999\n",
    "    try:\n",
    "        df['four_avg'] = (gi + gii + giii + giv) / 4\n",
    "    except:\n",
    "        df['four_avg'] = 9999\n",
    "    try:\n",
    "        df['five_avg'] = (gi + gii + giii + giv + gv) / 5\n",
    "    except:\n",
    "        df['five_avg'] = 9999\n",
    "    try:\n",
    "        df['six_avg'] = (gi + gii + giii + giv + gv + gvi) / 6\n",
    "    except:\n",
    "        df['six_avg'] = 9999\n",
    "    try:\n",
    "        df['seven_avg'] = (gi + gii + giii + giv + gv + gvi + gvii) / 7\n",
    "    except:\n",
    "        df['seven_avg'] = 9999\n",
    "        \n",
    "    # test scores\n",
    "    try:\n",
    "        df['two_test_avg'] = (ti + tii) / 2\n",
    "    except:\n",
    "        df['two_test_avg'] = 9999\n",
    "    try:\n",
    "        df['three_test_avg'] = (ti + tii + tiii) / 3\n",
    "    except:\n",
    "        df['three_test_avg'] = 9999\n",
    "    try:\n",
    "        df['four_test_avg'] = (ti + tii + tiii + tiv) / 4\n",
    "    except:\n",
    "        df['four_test_avg'] = 9999\n",
    "    try:\n",
    "        df['five_test_avg'] = (ti + tii + tiii + tiv + tv) / 5\n",
    "    except:\n",
    "        df['five_test_avg'] = 9999\n",
    "    try:\n",
    "        df['six_test_avg'] = (ti + tii + tiii + tiv + tv + tvi) / 6\n",
    "    except:\n",
    "        df['six_test_avg'] = 9999\n",
    "    try:\n",
    "        df['seven_test_avg'] = (ti + tii + tiii + tiv + tv + tvi + tvii) / 7\n",
    "    except:\n",
    "        df['seven_test_avg'] = 9999\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score Change Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slope(df):\n",
    "    # setting game acc variables\n",
    "    gi = df['last_score']\n",
    "    gii = df['last_two_score']\n",
    "    giii = df['last_three_score']\n",
    "    giv = df['last_four_score']\n",
    "    gv = df['last_five_score']\n",
    "    gvi = df['last_six_score']\n",
    "    gvii = df['last_seven_score']\n",
    "    # setting test acc variables\n",
    "    ti = df['last_test_score']\n",
    "    tii = df['last_two_test_score']\n",
    "    tiii = df['last_three_test_score']\n",
    "    tiv = df['last_four_test_score']\n",
    "    tv = df['last_five_test_score']\n",
    "    tvi = df['last_six_test_score']\n",
    "    tvii = df['last_seven_test_score']\n",
    "    \n",
    "    try:\n",
    "        df['gslope1'] = gi - gii\n",
    "    except:\n",
    "        df['gslope1'] = 9999\n",
    "    try:\n",
    "        df['gslope2'] = gi - giii\n",
    "    except:\n",
    "        df['gslope2'] = 9999\n",
    "    try:\n",
    "        df['gslope3'] = gi - giv\n",
    "    except:\n",
    "        df['gslope3'] = 9999\n",
    "    try:\n",
    "        df['gslope4'] = gi - gv\n",
    "    except:\n",
    "        df['gslope4'] = 9999\n",
    "    try:\n",
    "        df['tslope1'] = ti - tii\n",
    "    except:\n",
    "        df['tslope1'] = 9999\n",
    "    try:\n",
    "        df['tslope2'] = ti - tiii\n",
    "    except:\n",
    "        df['tslope2'] = 9999\n",
    "    try:\n",
    "        df['tslope3'] = ti - tiv\n",
    "    except:\n",
    "        df['tslope3'] = 9999\n",
    "    try:\n",
    "        df['tslope4'] = ti - tv\n",
    "    except:\n",
    "        df['tslope4'] = 9999\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time to finish last assessment / Speed (time per steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_speed(df):\n",
    "    tested_index = df[(df['type_Assessment'] == 1)].index\n",
    "    step_vals = df.loc[tested_index, 'session_steps'].to_dict()\n",
    "    time_vals = df.loc[tested_index, 'step_time'].to_dict()\n",
    "    \n",
    "    for i in range(len(tested_index)):\n",
    "        index = tested_index[i]\n",
    "        time = time_vals[index]\n",
    "        steps = step_vals[index]\n",
    "        df.loc[index, 'last_test_time'] = np.where((df.loc[index, 'new_install'] != 1), \n",
    "                                                   time, -10.0)\n",
    "        df.loc[index, 'last_test_speed'] = np.where((df.loc[index, 'new_install'] != 1), \n",
    "                                                   (time/steps), -10.0)\n",
    "        \n",
    "        index2 = tested_index[i-1]\n",
    "        time2 = time_vals[index2]\n",
    "        steps2 = step_vals[index2]\n",
    "        df.loc[index, 'last_test_time_2'] = np.where(((df.loc[index-1, 'new_install'] != 1) and\n",
    "                                                     (df.loc[index, 'new_install'] != 1)), \n",
    "                                                   time2, -10.0)\n",
    "        df.loc[index, 'last_test_speed_2'] = np.where(((df.loc[index-1, 'new_install'] != 1) and\n",
    "                                                     (df.loc[index, 'new_install'] != 1)), \n",
    "                                                   (time2/steps2), -10.0)\n",
    "        \n",
    "        index3 = tested_index[i-2]\n",
    "        time3 = time_vals[index3]\n",
    "        steps3 = step_vals[index3]\n",
    "        df.loc[index, 'last_test_time_3'] = np.where(((df.loc[index-2, 'new_install'] != 1) and\n",
    "                                                     (df.loc[index-1, 'new_install'] != 1) and\n",
    "                                                     (df.loc[index, 'new_install'] != 1)), \n",
    "                                                   time3, -10.0)\n",
    "        df.loc[index, 'last_test_speed_3'] = np.where(((df.loc[index-2, 'new_install'] != 1) and\n",
    "                                                      (df.loc[index-1, 'new_install'] != 1) and\n",
    "                                                     (df.loc[index, 'new_install'] != 1)), \n",
    "                                                    (time3/steps3), -10.0)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepping Final DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_functions(df, \n",
    "                  labels, \n",
    "                  type_ = ['train', 'test'], \n",
    "                  install_dict = [install_dict_test, install_dict_train],\n",
    "                  iterables = [train_iterables, test_iterables]):\n",
    "    df = fix_time(df)\n",
    "    df = filter_assessed(df, labels)\n",
    "    df = sort_by_time(df)\n",
    "    df = test_answers_df(df)\n",
    "    df = game_events(df)\n",
    "    df = new_session(df)\n",
    "    df = activity_df(df)\n",
    "    start_dict = start_time(df)\n",
    "    df = agg_session(df)\n",
    "    df = accuracy(df)\n",
    "    df = map_features(df)\n",
    "    df = map_install_start(df, install_dict, start_dict)\n",
    "    df = agg_install(df)\n",
    "    df = clean_up_aggs(df)\n",
    "    df = cumulative_accuracy(df, iterables)\n",
    "    df = drop_fill_cumsum(df)\n",
    "    df = cuma_proportions(df)\n",
    "    df = assessments_completed(df)\n",
    "    df = assessments_passed(df)\n",
    "    df = speed(df)\n",
    "    df = new_session_install(df)\n",
    "    \n",
    "    df = timedelta(df)\n",
    "    df = high_use(df)\n",
    "    df = last_test_scores(df)\n",
    "    df = last_game_scores(df)\n",
    "    df = map_nans(df)\n",
    "    df = average(df)\n",
    "    df = slope(df)\n",
    "    df = test_speed(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_train(df): \n",
    "    # dropping unnecessary columns\n",
    "    start_index = list(df.columns).index('installation_id')\n",
    "    new_cols = list(df.columns)[start_index:]\n",
    "    new_cols.append('game_session')\n",
    "    df = df[new_cols]\n",
    "    df = df[df['game_session'].isin(labeled_sessions)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_test(df):\n",
    "    start_index = list(df.columns).index('installation_id')\n",
    "    new_cols = list(df.columns)[start_index:]\n",
    "    new_cols.append('game_session')\n",
    "    df = df[new_cols]\n",
    "    df = df.groupby('installation_id').last()\n",
    "    df['installation_id'] = df.index\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = all_functions(train, \n",
    "                       labels, \n",
    "                       install_dict = install_dict_train,\n",
    "                       type_ = 'train', \n",
    "                       iterables = train_iterables)\n",
    "train3 = finish_train(train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train3.to_csv('./data/new_train2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = all_functions(test, \n",
    "                      test, \n",
    "                      install_dict = install_dict_test,\n",
    "                      type_ = 'test', \n",
    "                      iterables = test_iterables)\n",
    "test3 = finish_test(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test3.to_csv('./data/test3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
